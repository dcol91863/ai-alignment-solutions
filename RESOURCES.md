# Research Resources & References

## Foundational Papers

### Alignment Problem Framework
- Christiano, P., et al. (2019). "AI Safety and Reproducibility: What Have We Learned from Concrete Problems?"
- Russell, S. (2019). "Human Compatible: Artificial Intelligence and the Problem of Control"
- Hadfield-Menell, D., et al. (2016). "The Off-Switch Game"

### RLHF & Learning from Feedback
- Christiano, P., et al. (2017). "Deep reinforcement learning from human preferences"
- Ouyang, L., et al. (2022). "Training language models to follow instructions with human feedback"
- Ganguli, D., et al. (2022). "Red teaming language models to reduce harms"

### Constitutional AI
- Bai, Y., et al. (2022). "Constitutional AI: Harmlessness from AI Feedback"
- Bengio, Y., et al. (2023). "Alignment by Agreement"

### Interpretability
- Olah, C., et al. (2020). "The Building Blocks of Interpretability"
- Nanda, R., et al. (2023). "Progress Measures for Grokking via Mechanistic Interpretability"
- Burns, C., et al. (2022). "Discovering Latent Classes for Relation Extraction with Long-Range Dependencies"

### Formal Verification
- Gopinath, D., et al. (2021). "Certified Robustness to Adversarial Examples"
- Seshia, S.A., et al. (2022). "Towards Trustworthy AI System Development"

## Key Organizations & Publications

### Research Centers
| Organization | Focus | Website |
|---|---|---|
| Center for AI Safety | Safety, alignment | cais.org |
| Anthropic | Constitutional AI, interpretability | anthropic.com |
| OpenAI | Safety, alignment research | openai.com/research |
| DeepMind | Safety, alignment | deepmind.com |
| Redwood Research | Mechanistic interpretability | redwoodresearch.org |
| MIRI | Agent foundations, alignment | intelligence.org |

### Venues for Alignment Research
- NeurIPS Safety Workshop
- International Joint Conference on Artificial Intelligence (IJCAI)
- Alignment Research and Development (ARD) Workshop
- AI Safety Conference

### Notable Researchers
- Stuart Russell (UC Berkeley)
- Paul Christiano (OpenAI, MIRI)
- Yonatan Bai (Anthropic)
- Dario Amodei (Anthropic)
- Denny Zhou (Google DeepMind)

## Tools & Benchmarks

### Evaluation Benchmarks
- **TruthfulQA**: Evaluates truthfulness of language models
- **HellaSwag**: Tests common sense reasoning
- **BBQ**: Bias benchmark for bias evaluation
- **HumanEval**: Code generation evaluation
- **HELM**: Comprehensive language model evaluation

### Safety & Alignment Tools
- **Constitutional AI Framework**: Training framework for aligned models
- **Alignment Benchmark**: Systematic alignment evaluation
- **Red Teaming Frameworks**: Adversarial testing tools
- **Interpretability Tools**: Saliency maps, attention visualization

### Open Source Resources
- **Transformer Interpretability**: transformer-circuits.pub
- **Mechanistic Interpretability**: github.com/ArthurConmy/sae-demo
- **Constitutional AI Training**: github.com/anthropics/constitutional-ai

## Learning Resources

### Books
- "Human Compatible" by Stuart Russell
- "Superintelligence" by Nick Bostrom
- "The Alignment Problem" by Brian Christian

### Courses & Tutorials
- DeepMind Safety Course Series
- LessWrong AI Alignment Sequences
- Paul's Online Math Notes (for formal methods background)

### Blogs & Articles
- Anthropic Blog (anthropic.com/blog)
- DeepMind Blog (deepmind.com)
- Distill.pub (interactive machine learning explanations)
- LessWrong (lesswrong.com) - Philosophy and technical discussions

### Podcasts
- "Future of Life Institute Podcast" - AI safety discussions
- "The MIRI Podcast" - Agent foundations and alignment
- Research podcasts from universities working on alignment

## Getting Involved

### Contribute to Open Source Projects
- Help maintain alignment benchmarks
- Contribute interpretability tools
- Improve documentation
- Add evaluation frameworks

### Academic Routes
- Pursue PhD in related areas:
  - Machine learning safety
  - AI alignment
  - Formal verification
  - Human-computer interaction
- Work at research organizations
- Teach courses on AI safety

### Industry
- Join safety teams at major AI companies
- Help implement alignment techniques
- Build tools and infrastructure
- Red team as a service

### Policy & Advocacy
- Work with policymakers on AI governance
- Contribute to safety standards development
- Engage with international organizations
- Write about alignment for general audiences

## Reading List by Level

### Beginner
1. "What is AI Safety?" - Center for AI Safety
2. "An Introduction to AI Ethics" - Brent Mittelstadt
3. Common Sense in Artificial Intelligence course materials

### Intermediate
1. RLHF papers (Christiano et al., Ouyang et al.)
2. Constitutional AI papers (Bai et al.)
3. Mechanistic interpretability work (Olah et al., Nanda et al.)

### Advanced
1. Formal verification in ML (Seshia et al.)
2. Agent foundations and logical induction
3. Multi-agent game theory and mechanism design

### Cutting Edge
- Follow alignment researchers on Twitter/social media
- Attend alignment workshops and conferences
- Join research seminars at universities
- Read preprints on arXiv (cs.AI and cs.CY)

## Recommended Reading Order

**For the First Month:**
1. "Alignment: Why it's harder than it looks" (OpenPhil)
2. Constitutional AI papers
3. Why AI Safety Matters (Alignment Research Center)

**For Months 2-3:**
1. RLHF foundational papers
2. Interpretability blog posts (Distill)
3. TruthfulQA paper

**Month 4 and Beyond:**
1. Dive into specific research areas based on interest
2. Read mathematical foundations as needed
3. Engage with latest research and preprints

---

**Last updated**: February 2026
**Maintained by**: Community contributions welcome
